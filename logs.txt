
==> Audit <==
|-----------|--------------------------------|----------|-----------------------------|---------|---------------------|---------------------|
|  Command  |              Args              | Profile  |            User             | Version |     Start Time      |      End Time       |
|-----------|--------------------------------|----------|-----------------------------|---------|---------------------|---------------------|
| start     |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 13:57 +07 |                     |
| start     |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 13:58 +07 |                     |
| start     |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 22:49 +07 |                     |
| start     |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 22:51 +07 |                     |
| delete    |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 22:53 +07 | 28 May 25 22:53 +07 |
| start     |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 22:53 +07 |                     |
| start     | --driver=hyperv -v=1           | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 22:54 +07 |                     |
|           | --alsologtostderr              |          |                             |         |                     |                     |
| delete    | --all                          | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 22:54 +07 | 28 May 25 22:54 +07 |
| start     | --driver=docker                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 22:55 +07 |                     |
| delete    |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 22:58 +07 | 28 May 25 22:58 +07 |
| start     | --driver=docker                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 22:58 +07 |                     |
| delete    |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 23:00 +07 | 28 May 25 23:00 +07 |
| start     | --driver=docker                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 23:00 +07 | 28 May 25 23:04 +07 |
| dashboard |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 28 May 25 23:07 +07 |                     |
| dashboard |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:07 +07 |                     |
| start     | --driver=docker                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:10 +07 |                     |
| delete    |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:10 +07 | 30 May 25 00:10 +07 |
| start     | --driver=docker                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:10 +07 |                     |
| delete    |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:11 +07 | 30 May 25 00:11 +07 |
| start     | --driver=docker                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:11 +07 | 30 May 25 00:12 +07 |
| dashboard |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:12 +07 |                     |
| start     |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:18 +07 | 30 May 25 00:19 +07 |
| start     |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:21 +07 |                     |
| dashboard |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:21 +07 |                     |
| service   | hello-node                     | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:26 +07 |                     |
| dashboard |                                | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:41 +07 |                     |
| addons    | enable metricsserver           | minikube | LAPTOP-LT9PBK61\Figo Favian | v1.36.0 | 30 May 25 00:42 +07 |                     |
|-----------|--------------------------------|----------|-----------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/05/30 00:21:13
Running on machine: LAPTOP-LT9PBK61
Binary: Built with gc go1.24.0 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0530 00:21:13.826814    5796 out.go:345] Setting OutFile to fd 96 ...
I0530 00:21:13.827579    5796 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0530 00:21:13.827579    5796 out.go:358] Setting ErrFile to fd 100...
I0530 00:21:13.827579    5796 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0530 00:21:13.844567    5796 out.go:352] Setting JSON to false
I0530 00:21:13.849403    5796 start.go:130] hostinfo: {"hostname":"LAPTOP-LT9PBK61","uptime":13659,"bootTime":1748525614,"procs":276,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.5335 Build 22631.5335","kernelVersion":"10.0.22631.5335 Build 22631.5335","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"ea086344-518f-4867-924d-65fb9cec69b8"}
W0530 00:21:13.849403    5796 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0530 00:21:13.851082    5796 out.go:177] * minikube v1.36.0 on Microsoft Windows 11 Home Single Language 10.0.22631.5335 Build 22631.5335
I0530 00:21:13.852593    5796 notify.go:220] Checking for updates...
I0530 00:21:13.853160    5796 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0530 00:21:13.853160    5796 driver.go:404] Setting default libvirt URI to qemu:///system
I0530 00:21:13.920637    5796 docker.go:123] docker version: linux-27.5.1:Docker Desktop 4.38.0 (181591)
I0530 00:21:13.926600    5796 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0530 00:21:14.096461    5796 info.go:266] docker info: {ID:0dd81472-8dfc-4c0b-9e45-717e7cc1fc82 Containers:5 ContainersRunning:1 ContainersPaused:0 ContainersStopped:4 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:62 OomKillDisable:true NGoroutines:88 SystemTime:2025-05-29 17:21:14.084231268 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8187629568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0530 00:21:14.097561    5796 out.go:177] * Using the docker driver based on existing profile
I0530 00:21:14.098101    5796 start.go:304] selected driver: docker
I0530 00:21:14.098101    5796 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Figo Favian:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0530 00:21:14.098101    5796 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0530 00:21:14.109892    5796 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0530 00:21:14.306803    5796 info.go:266] docker info: {ID:0dd81472-8dfc-4c0b-9e45-717e7cc1fc82 Containers:5 ContainersRunning:1 ContainersPaused:0 ContainersStopped:4 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:62 OomKillDisable:true NGoroutines:88 SystemTime:2025-05-29 17:21:14.294619982 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8187629568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\Figo Favian\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0530 00:21:14.336891    5796 cni.go:84] Creating CNI manager for ""
I0530 00:21:14.336891    5796 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0530 00:21:14.336891    5796 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Figo Favian:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0530 00:21:14.337955    5796 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0530 00:21:14.338988    5796 cache.go:121] Beginning downloading kic base image for docker with docker
I0530 00:21:14.340200    5796 out.go:177] * Pulling base image v0.0.47 ...


==> Docker <==
May 29 17:19:29 minikube dockerd[6058]: time="2025-05-29T17:19:29.927322509Z" level=info msg="Daemon shutdown complete"
May 29 17:19:29 minikube dockerd[6058]: time="2025-05-29T17:19:29.927381495Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
May 29 17:19:29 minikube systemd[1]: docker.service: Deactivated successfully.
May 29 17:19:29 minikube systemd[1]: Stopped Docker Application Container Engine.
May 29 17:19:29 minikube systemd[1]: Starting Docker Application Container Engine...
May 29 17:19:29 minikube dockerd[6403]: time="2025-05-29T17:19:29.962963720Z" level=info msg="Starting up"
May 29 17:19:29 minikube dockerd[6403]: time="2025-05-29T17:19:29.964015675Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
May 29 17:19:29 minikube dockerd[6403]: time="2025-05-29T17:19:29.973324850Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
May 29 17:19:29 minikube dockerd[6403]: time="2025-05-29T17:19:29.982625509Z" level=info msg="[graphdriver] trying configured driver: overlay2"
May 29 17:19:30 minikube dockerd[6403]: time="2025-05-29T17:19:30.000864153Z" level=info msg="Loading containers: start."
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.265233046Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 355a94d9ec2a5689378df1712b37bff92bda4935fc9aad224764405296ac65a7], retrying...."
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.317819666Z" level=info msg="Loading containers: done."
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.334918198Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.334963223Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.334968261Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.334971552Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.334992302Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.335026839Z" level=info msg="Initializing buildkit"
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.406360793Z" level=info msg="Completed buildkit initialization"
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.413552006Z" level=info msg="Daemon has completed initialization"
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.413708664Z" level=info msg="API listen on /var/run/docker.sock"
May 29 17:19:32 minikube dockerd[6403]: time="2025-05-29T17:19:32.413736351Z" level=info msg="API listen on [::]:2376"
May 29 17:19:32 minikube systemd[1]: Started Docker Application Container Engine.
May 29 17:19:32 minikube cri-dockerd[1783]: time="2025-05-29T17:19:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-5txqq_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9e2c5e6b0d0f790dd58eeee6039aeabdfce31045c83b9996c71758e8a6d04442\""
May 29 17:19:32 minikube cri-dockerd[1783]: time="2025-05-29T17:19:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-6znv6_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6498b837f33ab293ea8882eaed495664837a737f896f5554f6cbf02efb4c8334\""
May 29 17:19:32 minikube cri-dockerd[1783]: time="2025-05-29T17:19:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-7779f9b69b-xlvjx_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"03aa3f07b4239a59e8ada72248110949f84fad42f129d05c59ad6783ccbb778a\""
May 29 17:19:32 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
May 29 17:19:32 minikube systemd[1]: cri-docker.service: Deactivated successfully.
May 29 17:19:32 minikube systemd[1]: cri-docker.service: Unit process 6659 (firewall) remains running after unit stopped.
May 29 17:19:32 minikube systemd[1]: cri-docker.service: Unit process 6673 (firewall) remains running after unit stopped.
May 29 17:19:32 minikube systemd[1]: cri-docker.service: Unit process 6680 (iptables) remains running after unit stopped.
May 29 17:19:32 minikube systemd[1]: cri-docker.service: Unit process 6682 (iptables) remains running after unit stopped.
May 29 17:19:32 minikube systemd[1]: cri-docker.service: Unit process 6688 (firewall) remains running after unit stopped.
May 29 17:19:32 minikube systemd[1]: cri-docker.service: Unit process 6693 (iptables) remains running after unit stopped.
May 29 17:19:32 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
May 29 17:19:33 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
May 29 17:19:33 minikube cri-dockerd[6774]: time="2025-05-29T17:19:33Z" level=info msg="Starting cri-dockerd dev (HEAD)"
May 29 17:19:33 minikube cri-dockerd[6774]: time="2025-05-29T17:19:33Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 29 17:19:33 minikube cri-dockerd[6774]: time="2025-05-29T17:19:33Z" level=info msg="Start docker client with request timeout 0s"
May 29 17:19:33 minikube cri-dockerd[6774]: time="2025-05-29T17:19:33Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 29 17:19:33 minikube cri-dockerd[6774]: time="2025-05-29T17:19:33Z" level=info msg="Loaded network plugin cni"
May 29 17:19:33 minikube cri-dockerd[6774]: time="2025-05-29T17:19:33Z" level=info msg="Docker cri networking managed by network plugin cni"
May 29 17:19:33 minikube cri-dockerd[6774]: time="2025-05-29T17:19:33Z" level=info msg="Setting cgroupDriver cgroupfs"
May 29 17:19:33 minikube cri-dockerd[6774]: time="2025-05-29T17:19:33Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 29 17:19:33 minikube cri-dockerd[6774]: time="2025-05-29T17:19:33Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 29 17:19:33 minikube cri-dockerd[6774]: time="2025-05-29T17:19:33Z" level=info msg="Start cri-dockerd grpc backend"
May 29 17:19:33 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 29 17:19:34 minikube cri-dockerd[6774]: time="2025-05-29T17:19:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cf77864114ad792ccf04da478d3fa3654ebb548e33c40a05bc498acb743a68fd/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 29 17:19:34 minikube cri-dockerd[6774]: time="2025-05-29T17:19:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/282b042872c62114dbab73148e2c8d5041bf173fa85c5546c69cda7d80df3432/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 29 17:19:34 minikube cri-dockerd[6774]: time="2025-05-29T17:19:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/82b624e8ab2b443ec553aa60d30056dbfe316e02c2dc0052812b741efcf84a86/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 29 17:19:34 minikube cri-dockerd[6774]: time="2025-05-29T17:19:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/abdf896247354907413aee5d8b6257d8a68fa67953b0bd6ba017074973c38e4d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 29 17:19:34 minikube cri-dockerd[6774]: time="2025-05-29T17:19:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/db9889e566698eea9fa91a5ffee87ce2807ac3a2b6e6298432f673287e7ecc59/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 29 17:19:34 minikube cri-dockerd[6774]: time="2025-05-29T17:19:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/feeeb82b175a8ef3cde8bf407555e66241b9cc285458a8335e425dc8e9106246/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 29 17:19:34 minikube cri-dockerd[6774]: time="2025-05-29T17:19:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/13861c4337f8a81d5f91c0afbf916889ad103addc30cfefcd80fa9e3b4f6ac5c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 29 17:19:36 minikube dockerd[6403]: time="2025-05-29T17:19:36.779880812Z" level=info msg="ignoring event" container=4b781dd0325b23ad3b7850e7861093cce3e35160ed103217e28855562d509434 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 29 17:19:39 minikube cri-dockerd[6774]: time="2025-05-29T17:19:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e3d12726b6de250786da1215f3df402f39e10e316a233392ecf82ab8e56ee5a0/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 29 17:19:39 minikube cri-dockerd[6774]: time="2025-05-29T17:19:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ab98b479724827afebc6b004f8bbcc4e1e4bd6a320cb4574acbb24c49b9dcac4/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 29 17:22:19 minikube cri-dockerd[6774]: time="2025-05-29T17:22:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/afad6e9ebb97d1c91e9c3c265208d523e17913dd03e60694fc0a0c80c67336e3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 29 17:22:30 minikube cri-dockerd[6774]: time="2025-05-29T17:22:30Z" level=info msg="Pulling image registry.k8s.io/e2e-test-images/agnhost:2.39: 3d2bf5363f5a: Downloading [=============================>                     ]  8.061MB/13.79MB"
May 29 17:22:39 minikube cri-dockerd[6774]: time="2025-05-29T17:22:39Z" level=info msg="Stop pulling image registry.k8s.io/e2e-test-images/agnhost:2.39: Status: Downloaded newer image for registry.k8s.io/e2e-test-images/agnhost:2.39"


==> container status <==
CONTAINER           IMAGE                                                                                                             CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
9729dbf7e5525       registry.k8s.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e   20 minutes ago      Running             agnhost                     0                   afad6e9ebb97d       hello-node-c74958b5d-q6rxz
a59b13db9baef       6e38f40d628db                                                                                                     23 minutes ago      Running             storage-provisioner         3                   282b042872c62       storage-provisioner
774e649c30c90       115053965e86b                                                                                                     23 minutes ago      Running             dashboard-metrics-scraper   1                   ab98b47972482       dashboard-metrics-scraper-5d59dccf9b-5txqq
94981d1fd38c6       07655ddf2eebe                                                                                                     23 minutes ago      Running             kubernetes-dashboard        1                   e3d12726b6de2       kubernetes-dashboard-7779f9b69b-xlvjx
5e418802e0027       ef43894fa110c                                                                                                     23 minutes ago      Running             kube-controller-manager     1                   cf77864114ad7       kube-controller-manager-minikube
664cf09132d9b       1cf5f116067c6                                                                                                     23 minutes ago      Running             coredns                     1                   feeeb82b175a8       coredns-674b8bbfcf-6znv6
d0cb12e4756c4       499038711c081                                                                                                     23 minutes ago      Running             etcd                        1                   13861c4337f8a       etcd-minikube
fc5f47f868eb1       c6ab243b29f82                                                                                                     23 minutes ago      Running             kube-apiserver              1                   db9889e566698       kube-apiserver-minikube
968ee71859c4d       b79c189b052cd                                                                                                     23 minutes ago      Running             kube-proxy                  1                   abdf896247354       kube-proxy-4qqb8
0d3d74d759543       398c985c0d950                                                                                                     23 minutes ago      Running             kube-scheduler              1                   82b624e8ab2b4       kube-scheduler-minikube
4b781dd0325b2       6e38f40d628db                                                                                                     23 minutes ago      Exited              storage-provisioner         2                   282b042872c62       storage-provisioner
87f6e3c318e4c       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c              29 minutes ago      Exited              dashboard-metrics-scraper   0                   9e2c5e6b0d0f7       dashboard-metrics-scraper-5d59dccf9b-5txqq
529a489c35cb2       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93                    30 minutes ago      Exited              kubernetes-dashboard        0                   03aa3f07b4239       kubernetes-dashboard-7779f9b69b-xlvjx
2e8a5b180f1c3       1cf5f116067c6                                                                                                     31 minutes ago      Exited              coredns                     0                   6498b837f33ab       coredns-674b8bbfcf-6znv6
91ecfe17b909e       b79c189b052cd                                                                                                     31 minutes ago      Exited              kube-proxy                  0                   f6dce8d28f01b       kube-proxy-4qqb8
18901915558d4       ef43894fa110c                                                                                                     31 minutes ago      Exited              kube-controller-manager     0                   3531d187a3fe0       kube-controller-manager-minikube
23ec27fff8394       499038711c081                                                                                                     31 minutes ago      Exited              etcd                        0                   8eeb1c89b7eb6       etcd-minikube
2ffddb0f7c78f       398c985c0d950                                                                                                     31 minutes ago      Exited              kube-scheduler              0                   9a0f3dda5d3ec       kube-scheduler-minikube
c921d9d7a681e       c6ab243b29f82                                                                                                     31 minutes ago      Exited              kube-apiserver              0                   aba1427a57edf       kube-apiserver-minikube


==> coredns [2e8a5b180f1c] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:35483 - 34086 "HINFO IN 269040929817901585.2663065541538703794. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.111122538s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [664cf09132d9] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:46435 - 59060 "HINFO IN 7176753222311317887.4937805525597817957. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.096429534s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_30T00_12_15_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 29 May 2025 17:12:12 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 29 May 2025 17:43:22 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 29 May 2025 17:43:12 +0000   Thu, 29 May 2025 17:12:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 29 May 2025 17:43:12 +0000   Thu, 29 May 2025 17:12:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 29 May 2025 17:43:12 +0000   Thu, 29 May 2025 17:12:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 29 May 2025 17:43:12 +0000   Thu, 29 May 2025 17:12:13 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7995732Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7995732Ki
  pods:               110
System Info:
  Machine ID:                 8d00a8bdb9fe45bd904a670b845f33f4
  System UUID:                8d00a8bdb9fe45bd904a670b845f33f4
  Boot ID:                    4619b105-9b17-4477-b104-d77b5fbfb1fe
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     hello-node-c74958b5d-q6rxz                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m
  kube-system                 coredns-674b8bbfcf-6znv6                      100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     31m
  kube-system                 etcd-minikube                                 100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         31m
  kube-system                 kube-apiserver-minikube                       250m (1%)     0 (0%)      0 (0%)           0 (0%)         31m
  kube-system                 kube-controller-manager-minikube              200m (1%)     0 (0%)      0 (0%)           0 (0%)         31m
  kube-system                 kube-proxy-4qqb8                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m
  kube-system                 kube-scheduler-minikube                       100m (0%)     0 (0%)      0 (0%)           0 (0%)         31m
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-5txqq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         30m
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-xlvjx         0 (0%)        0 (0%)      0 (0%)           0 (0%)         30m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age   From             Message
  ----     ------                             ----  ----             -------
  Normal   Starting                           31m   kube-proxy       
  Normal   Starting                           23m   kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  31m   kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           31m   kubelet          Starting kubelet.
  Warning  CgroupV1                           31m   kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            31m   kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            31m   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              31m   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               31m   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     31m   node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode                     23m   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May29 17:10] PCI: Fatal: No config space access function found
[  +0.017439] PCI: System does not support PCI
[  +0.186235] kvm: already loaded the other module
[  +2.069468] FS-Cache: Duplicate cookie detected
[  +0.000985] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.001644] FS-Cache: O-cookie d=00000000f75ce013{9P.session} n=00000000dc35135e
[  +0.001289] FS-Cache: O-key=[10] '34323934393337353236'
[  +0.000308] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000466] FS-Cache: N-cookie d=00000000f75ce013{9P.session} n=0000000043554d79
[  +0.000527] FS-Cache: N-key=[10] '34323934393337353236'
[  +1.166264] /sbin/ldconfig: 
[  +0.000003] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.337504] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.013711] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Jakarta not found. Is the tzdata package installed?
[May29 17:11] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.009854] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001817] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000800] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000766] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001975] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000954] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001217] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001309] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.300574] netlink: 'init': attribute type 4 has an invalid length.
[  +1.760212] WSL (205) ERROR: CheckConnection: getaddrinfo() failed: -5
[May29 17:12] tmpfs: Unknown parameter 'noswap'
[  +7.009419] tmpfs: Unknown parameter 'noswap'


==> etcd [23ec27fff839] <==
{"level":"info","ts":"2025-05-29T17:12:09.917733Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-29T17:12:09.917164Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-29T17:12:09.918777Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-29T17:12:09.918865Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-29T17:12:09.919997Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-05-29T17:12:09.920333Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-05-29T17:12:10.431795Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-05-29T17:12:10.431843Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-05-29T17:12:10.431857Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-05-29T17:12:10.431877Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-05-29T17:12:10.431907Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-29T17:12:10.431914Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-05-29T17:12:10.431920Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-29T17:12:10.434908Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-05-29T17:12:10.435012Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-29T17:12:10.435059Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-29T17:12:10.435224Z","caller":"etcdserver/server.go:2697","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-29T17:12:10.435475Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-05-29T17:12:10.435583Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-05-29T17:12:10.435628Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-29T17:12:10.436065Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-05-29T17:12:10.438219Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-29T17:12:10.438796Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-05-29T17:12:10.441525Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-29T17:12:10.441679Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-29T17:12:10.441724Z","caller":"etcdserver/server.go:2721","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"warn","ts":"2025-05-29T17:12:12.828703Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.920754ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/certificatesigningrequests\" limit:1 ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-05-29T17:12:12.828883Z","caller":"traceutil/trace.go:171","msg":"trace[2026246745] range","detail":"{range_begin:/registry/certificatesigningrequests; range_end:; response_count:0; response_revision:14; }","duration":"102.192357ms","start":"2025-05-29T17:12:12.726666Z","end":"2025-05-29T17:12:12.828858Z","steps":["trace[2026246745] 'agreement among raft nodes before linearized reading'  (duration: 91.12075ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:12.926985Z","caller":"traceutil/trace.go:171","msg":"trace[450798056] transaction","detail":"{read_only:false; response_revision:17; number_of_response:1; }","duration":"109.484841ms","start":"2025-05-29T17:12:12.817467Z","end":"2025-05-29T17:12:12.926951Z","steps":["trace[450798056] 'process raft request'  (duration: 98.900271ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:12.927160Z","caller":"traceutil/trace.go:171","msg":"trace[1768098081] transaction","detail":"{read_only:false; response_revision:18; number_of_response:1; }","duration":"109.570611ms","start":"2025-05-29T17:12:12.817533Z","end":"2025-05-29T17:12:12.927104Z","steps":["trace[1768098081] 'process raft request'  (duration: 109.202606ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:12.927209Z","caller":"traceutil/trace.go:171","msg":"trace[1353781100] transaction","detail":"{read_only:false; response_revision:21; number_of_response:1; }","duration":"109.496945ms","start":"2025-05-29T17:12:12.817684Z","end":"2025-05-29T17:12:12.927181Z","steps":["trace[1353781100] 'process raft request'  (duration: 109.320887ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:12.927460Z","caller":"traceutil/trace.go:171","msg":"trace[1546329633] transaction","detail":"{read_only:false; response_revision:22; number_of_response:1; }","duration":"109.44388ms","start":"2025-05-29T17:12:12.817983Z","end":"2025-05-29T17:12:12.927427Z","steps":["trace[1546329633] 'process raft request'  (duration: 109.042891ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:12.927587Z","caller":"traceutil/trace.go:171","msg":"trace[789284689] linearizableReadLoop","detail":"{readStateIndex:24; appliedIndex:17; }","duration":"109.807069ms","start":"2025-05-29T17:12:12.817768Z","end":"2025-05-29T17:12:12.927575Z","steps":["trace[789284689] 'read index received'  (duration: 10.233505ms)","trace[789284689] 'applied index is now lower than readState.Index'  (duration: 99.571723ms)"],"step_count":2}
{"level":"info","ts":"2025-05-29T17:12:12.927477Z","caller":"traceutil/trace.go:171","msg":"trace[1193924249] transaction","detail":"{read_only:false; response_revision:25; number_of_response:1; }","duration":"100.577334ms","start":"2025-05-29T17:12:12.826856Z","end":"2025-05-29T17:12:12.927434Z","steps":["trace[1193924249] 'process raft request'  (duration: 100.263771ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:12.927684Z","caller":"traceutil/trace.go:171","msg":"trace[2145291768] transaction","detail":"{read_only:false; response_revision:19; number_of_response:1; }","duration":"110.111696ms","start":"2025-05-29T17:12:12.817550Z","end":"2025-05-29T17:12:12.927662Z","steps":["trace[2145291768] 'process raft request'  (duration: 109.391458ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:12.927734Z","caller":"traceutil/trace.go:171","msg":"trace[1655100724] transaction","detail":"{read_only:false; response_revision:20; number_of_response:1; }","duration":"110.097015ms","start":"2025-05-29T17:12:12.817619Z","end":"2025-05-29T17:12:12.927717Z","steps":["trace[1655100724] 'process raft request'  (duration: 109.363561ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:12.927513Z","caller":"traceutil/trace.go:171","msg":"trace[1477094991] transaction","detail":"{read_only:false; response_revision:24; number_of_response:1; }","duration":"104.072716ms","start":"2025-05-29T17:12:12.823421Z","end":"2025-05-29T17:12:12.927494Z","steps":["trace[1477094991] 'process raft request'  (duration: 103.650331ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:12.927661Z","caller":"traceutil/trace.go:171","msg":"trace[453016696] transaction","detail":"{read_only:false; response_revision:23; number_of_response:1; }","duration":"104.614672ms","start":"2025-05-29T17:12:12.823020Z","end":"2025-05-29T17:12:12.927635Z","steps":["trace[453016696] 'process raft request'  (duration: 104.025639ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-29T17:12:12.928016Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.39504ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/kube-system/extension-apiserver-authentication\" limit:1 ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-05-29T17:12:12.928083Z","caller":"traceutil/trace.go:171","msg":"trace[273372704] range","detail":"{range_begin:/registry/configmaps/kube-system/extension-apiserver-authentication; range_end:; response_count:0; response_revision:27; }","duration":"111.520315ms","start":"2025-05-29T17:12:12.816543Z","end":"2025-05-29T17:12:12.928063Z","steps":["trace[273372704] 'agreement among raft nodes before linearized reading'  (duration: 111.407501ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-29T17:12:13.016060Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"198.58772ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-05-29T17:12:13.016577Z","caller":"traceutil/trace.go:171","msg":"trace[964306613] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:27; }","duration":"199.457359ms","start":"2025-05-29T17:12:12.817086Z","end":"2025-05-29T17:12:13.016543Z","steps":["trace[964306613] 'agreement among raft nodes before linearized reading'  (duration: 110.590421ms)","trace[964306613] 'filter and sort the key-value pairs'  (duration: 87.973564ms)"],"step_count":2}
{"level":"info","ts":"2025-05-29T17:12:13.117705Z","caller":"traceutil/trace.go:171","msg":"trace[2114506495] transaction","detail":"{read_only:false; response_revision:28; number_of_response:1; }","duration":"101.407944ms","start":"2025-05-29T17:12:13.016259Z","end":"2025-05-29T17:12:13.117667Z","steps":["trace[2114506495] 'process raft request'  (duration: 101.140788ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:13.127615Z","caller":"traceutil/trace.go:171","msg":"trace[1586014520] transaction","detail":"{read_only:false; response_revision:35; number_of_response:1; }","duration":"103.172967ms","start":"2025-05-29T17:12:13.024354Z","end":"2025-05-29T17:12:13.127527Z","steps":["trace[1586014520] 'process raft request'  (duration: 103.149068ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:13.127924Z","caller":"traceutil/trace.go:171","msg":"trace[1029570064] transaction","detail":"{read_only:false; response_revision:32; number_of_response:1; }","duration":"111.285188ms","start":"2025-05-29T17:12:13.016628Z","end":"2025-05-29T17:12:13.127914Z","steps":["trace[1029570064] 'process raft request'  (duration: 110.819257ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:13.128090Z","caller":"traceutil/trace.go:171","msg":"trace[623140864] transaction","detail":"{read_only:false; response_revision:33; number_of_response:1; }","duration":"111.366202ms","start":"2025-05-29T17:12:13.016718Z","end":"2025-05-29T17:12:13.128084Z","steps":["trace[623140864] 'process raft request'  (duration: 110.746928ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:13.128107Z","caller":"traceutil/trace.go:171","msg":"trace[315966847] transaction","detail":"{read_only:false; response_revision:30; number_of_response:1; }","duration":"111.647734ms","start":"2025-05-29T17:12:13.016452Z","end":"2025-05-29T17:12:13.128100Z","steps":["trace[315966847] 'process raft request'  (duration: 110.949912ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:13.128169Z","caller":"traceutil/trace.go:171","msg":"trace[338702355] transaction","detail":"{read_only:false; response_revision:34; number_of_response:1; }","duration":"111.379578ms","start":"2025-05-29T17:12:13.016784Z","end":"2025-05-29T17:12:13.128164Z","steps":["trace[338702355] 'process raft request'  (duration: 110.70233ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:13.127921Z","caller":"traceutil/trace.go:171","msg":"trace[1246664063] transaction","detail":"{read_only:false; response_revision:31; number_of_response:1; }","duration":"111.282435ms","start":"2025-05-29T17:12:13.016567Z","end":"2025-05-29T17:12:13.127849Z","steps":["trace[1246664063] 'process raft request'  (duration: 110.862204ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:12:13.128265Z","caller":"traceutil/trace.go:171","msg":"trace[1297024212] transaction","detail":"{read_only:false; response_revision:29; number_of_response:1; }","duration":"111.812276ms","start":"2025-05-29T17:12:13.016433Z","end":"2025-05-29T17:12:13.128245Z","steps":["trace[1297024212] 'process raft request'  (duration: 110.88958ms)"],"step_count":1}
{"level":"info","ts":"2025-05-29T17:19:17.079761Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-05-29T17:19:17.079888Z","caller":"embed/etcd.go:408","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-05-29T17:19:24.084615Z","caller":"etcdserver/server.go:1546","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"warn","ts":"2025-05-29T17:19:24.084777Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-29T17:19:24.084760Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-29T17:19:24.084898Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-29T17:19:24.084911Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"info","ts":"2025-05-29T17:19:24.092922Z","caller":"embed/etcd.go:613","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-29T17:19:24.093161Z","caller":"embed/etcd.go:618","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-29T17:19:24.093179Z","caller":"embed/etcd.go:410","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [d0cb12e4756c] <==
{"level":"warn","ts":"2025-05-29T17:19:37.393151Z","caller":"etcdmain/config.go:389","msg":"--proxy-refresh-interval is deprecated in 3.5 and will be decommissioned in 3.6."}
{"level":"info","ts":"2025-05-29T17:19:37.393221Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-05-29T17:19:37.393416Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-05-29T17:19:37.393480Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-05-29T17:19:37.393524Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-05-29T17:19:37.393594Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-05-29T17:19:37.395940Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-05-29T17:19:37.396400Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-05-29T17:19:37.481459Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"84.491925ms"}
{"level":"info","ts":"2025-05-29T17:19:37.492449Z","caller":"etcdserver/server.go:534","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2025-05-29T17:19:37.578670Z","caller":"etcdserver/raft.go:541","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":898}
{"level":"info","ts":"2025-05-29T17:19:37.580458Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-05-29T17:19:37.580667Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2025-05-29T17:19:37.580738Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 898, applied: 0, lastindex: 898, lastterm: 2]"}
{"level":"warn","ts":"2025-05-29T17:19:37.597064Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-05-29T17:19:37.677328Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":802}
{"level":"info","ts":"2025-05-29T17:19:37.677500Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":898}
{"level":"info","ts":"2025-05-29T17:19:37.687458Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-05-29T17:19:37.697914Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-05-29T17:19:37.699040Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-05-29T17:19:37.699193Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-05-29T17:19:37.700825Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-29T17:19:37.703755Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-05-29T17:19:37.704109Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-05-29T17:19:37.704150Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-05-29T17:19:37.704252Z","caller":"etcdserver/server.go:775","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-05-29T17:19:37.704388Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-29T17:19:37.704439Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-29T17:19:37.704447Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-29T17:19:37.704680Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-29T17:19:37.704710Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-29T17:19:37.705320Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-05-29T17:19:37.705379Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-05-29T17:19:37.776312Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-29T17:19:37.776382Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-29T17:19:38.681940Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-05-29T17:19:38.682045Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-05-29T17:19:38.682088Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-29T17:19:38.682122Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-05-29T17:19:38.682198Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-05-29T17:19:38.682261Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-05-29T17:19:38.682272Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-05-29T17:19:38.686780Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-29T17:19:38.686815Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-29T17:19:38.686771Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-05-29T17:19:38.687189Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-05-29T17:19:38.687225Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-05-29T17:19:38.687951Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-29T17:19:38.687974Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-29T17:19:38.688616Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-05-29T17:19:38.688669Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-05-29T17:29:40.588226Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1194}
{"level":"info","ts":"2025-05-29T17:29:40.621234Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1194,"took":"32.31863ms","hash":1178817585,"current-db-size-bytes":3276800,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1662976,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-05-29T17:29:40.621366Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1178817585,"revision":1194,"compact-revision":-1}
{"level":"info","ts":"2025-05-29T17:34:40.581011Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1440}
{"level":"info","ts":"2025-05-29T17:34:40.587907Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1440,"took":"6.023544ms","hash":4175919226,"current-db-size-bytes":3276800,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1835008,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-05-29T17:34:40.587998Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4175919226,"revision":1440,"compact-revision":1194}
{"level":"info","ts":"2025-05-29T17:39:40.566319Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1680}
{"level":"info","ts":"2025-05-29T17:39:40.570162Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1680,"took":"3.561949ms","hash":2770872382,"current-db-size-bytes":3276800,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1703936,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-05-29T17:39:40.570233Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2770872382,"revision":1680,"compact-revision":1440}


==> kernel <==
 17:43:31 up 32 min,  0 users,  load average: 0.05, 0.15, 0.23
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [c921d9d7a681] <==
W0529 17:19:22.712092       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:22.780193       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:22.813136       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:22.823004       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:22.932097       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:23.386390       1 logging.go:55] [core] [Channel #217 SubChannel #218]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.322723       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.346435       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.427799       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.516773       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.575265       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.698664       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.701985       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.797048       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.825277       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.899428       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.899483       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.928834       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.928921       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.928953       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.964104       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:25.978843       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.015871       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.033526       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.051702       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.073002       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.091154       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.097273       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.116686       1 logging.go:55] [core] [Channel #217 SubChannel #218]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.151230       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.162178       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.173075       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.185554       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.223586       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.235298       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.251255       1 logging.go:55] [core] [Channel #15 SubChannel #17]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.313112       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.327947       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.389295       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.445533       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.452797       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.454573       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.474182       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.501265       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.521541       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.629588       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.639392       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.703473       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.720513       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.759212       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.765881       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.827508       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.856567       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.876788       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.894374       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.951911       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.967640       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:26.978274       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:27.073633       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0529 17:19:27.087782       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [fc5f47f868eb] <==
I0529 17:19:41.990806       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I0529 17:19:41.990840       1 shared_informer.go:350] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I0529 17:19:41.990900       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0529 17:19:41.990914       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0529 17:19:41.990916       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0529 17:19:41.990941       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0529 17:19:41.991311       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I0529 17:19:41.991550       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0529 17:19:41.991823       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0529 17:19:41.991904       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0529 17:19:41.991954       1 aggregator.go:169] waiting for initial CRD sync...
I0529 17:19:41.991978       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0529 17:19:41.993188       1 repairip.go:200] Starting ipallocator-repair-controller
I0529 17:19:41.993224       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0529 17:19:42.006830       1 naming_controller.go:299] Starting NamingConditionController
I0529 17:19:42.006901       1 controller.go:142] Starting OpenAPI controller
I0529 17:19:42.006922       1 controller.go:90] Starting OpenAPI V3 controller
I0529 17:19:42.007181       1 establishing_controller.go:81] Starting EstablishingController
I0529 17:19:42.007204       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0529 17:19:42.007215       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0529 17:19:42.007227       1 crd_finalizer.go:269] Starting CRDFinalizer
I0529 17:19:42.007282       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0529 17:19:42.007288       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0529 17:19:42.008251       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0529 17:19:42.008260       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0529 17:19:42.008264       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0529 17:19:42.008293       1 aggregator.go:171] initial CRD sync complete...
I0529 17:19:42.008298       1 autoregister_controller.go:144] Starting autoregister controller
I0529 17:19:42.008303       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0529 17:19:42.008539       1 cache.go:39] Caches are synced for autoregister controller
I0529 17:19:42.176049       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0529 17:19:42.176095       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0529 17:19:42.176131       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0529 17:19:42.176065       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0529 17:19:42.176137       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0529 17:19:42.176312       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0529 17:19:42.176344       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0529 17:19:42.176344       1 cache.go:39] Caches are synced for LocalAvailability controller
I0529 17:19:42.176400       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0529 17:19:42.176881       1 policy_source.go:240] refreshing policies
I0529 17:19:42.176514       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0529 17:19:42.177020       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0529 17:19:42.177029       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0529 17:19:42.177020       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0529 17:19:42.194875       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0529 17:19:42.276394       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
E0529 17:19:42.376248       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0529 17:19:42.994875       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
E0529 17:19:43.589775       1 storage.go:479] "Unhandled Error" err="Address {10.244.0.4  0xc0066b5f00 0xc0026b9340} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.7}] vs 10.244.0.4 (kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-5txqq))" logger="UnhandledError"
E0529 17:19:43.592033       1 storage.go:489] "Unhandled Error" err="Failed to find a valid address, skipping subset: &{[{10.244.0.4  0xc0066b5f00 0xc0026b9340}] [] [{ 8000 TCP <nil>}]}" logger="UnhandledError"
I0529 17:19:44.007449       1 controller.go:667] quota admission added evaluator for: endpoints
I0529 17:19:45.054743       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0529 17:19:46.483271       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0529 17:19:46.670081       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0529 17:19:46.721547       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0529 17:19:46.818793       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0529 17:25:10.960864       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0529 17:25:10.962622       1 alloc.go:328] "allocated clusterIPs" service="default/hello-node" clusterIPs={"IPv4":"10.104.201.179"}
I0529 17:29:41.960983       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0529 17:39:41.916705       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [18901915558d] <==
I0529 17:12:19.146268       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0529 17:12:19.149338       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0529 17:12:19.173798       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0529 17:12:19.174586       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0529 17:12:19.174706       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0529 17:12:19.176288       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0529 17:12:19.176451       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0529 17:12:19.176512       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0529 17:12:19.177941       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0529 17:12:19.178070       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0529 17:12:19.179501       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0529 17:12:19.181108       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0529 17:12:19.227386       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0529 17:12:19.229239       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0529 17:12:19.231476       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0529 17:12:19.231540       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0529 17:12:19.236813       1 shared_informer.go:357] "Caches are synced" controller="node"
I0529 17:12:19.236898       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0529 17:12:19.236921       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0529 17:12:19.236927       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0529 17:12:19.236934       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0529 17:12:19.238603       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0529 17:12:19.245598       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0529 17:12:19.247134       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0529 17:12:19.327825       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0529 17:12:19.330083       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0529 17:12:19.520197       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0529 17:12:19.525428       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0529 17:12:19.528438       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0529 17:12:19.528678       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0529 17:12:19.529989       1 shared_informer.go:357] "Caches are synced" controller="job"
I0529 17:12:19.530439       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0529 17:12:19.530461       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0529 17:12:19.530867       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0529 17:12:19.530984       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0529 17:12:19.531005       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0529 17:12:19.531080       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0529 17:12:19.531375       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0529 17:12:19.531573       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0529 17:12:19.540486       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0529 17:12:19.540600       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0529 17:12:19.561718       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0529 17:12:19.570523       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0529 17:12:19.574890       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0529 17:12:19.574944       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0529 17:12:19.579847       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0529 17:12:19.579899       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0529 17:12:19.580347       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0529 17:12:19.582461       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0529 17:12:19.949901       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0529 17:12:19.954259       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0529 17:12:19.954281       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0529 17:12:19.954286       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
E0529 17:12:57.245063       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0529 17:12:57.251991       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0529 17:12:57.252058       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0529 17:12:57.257545       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0529 17:12:57.259285       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0529 17:12:57.263007       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0529 17:12:57.266405       1 replica_set.go:562] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"


==> kube-controller-manager [5e418802e002] <==
I0529 17:19:46.063667       1 graph_builder.go:351] "Running" logger="garbage-collector-controller" component="GraphBuilder"
I0529 17:19:46.063775       1 controllermanager.go:778] "Started controller" controller="garbage-collector-controller"
I0529 17:19:46.113478       1 controllermanager.go:778] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0529 17:19:46.113520       1 controllermanager.go:736] "Skipping a cloud provider controller" controller="service-lb-controller"
I0529 17:19:46.113528       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="volumeattributesclass-protection-controller" requiredFeatureGates=["VolumeAttributesClass"]
I0529 17:19:46.113548       1 controllermanager.go:756] "Warning: skipping controller" controller="storage-version-migrator-controller"
I0529 17:19:46.113763       1 cleaner.go:83] "Starting CSR cleaner controller" logger="certificatesigningrequest-cleaner-controller"
I0529 17:19:46.118731       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0529 17:19:46.126243       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0529 17:19:46.136756       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0529 17:19:46.136818       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0529 17:19:46.136835       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0529 17:19:46.138368       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0529 17:19:46.139953       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0529 17:19:46.141606       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0529 17:19:46.143132       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0529 17:19:46.149066       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0529 17:19:46.150324       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0529 17:19:46.162530       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0529 17:19:46.163110       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0529 17:19:46.163661       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0529 17:19:46.164650       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0529 17:19:46.168539       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0529 17:19:46.184736       1 shared_informer.go:357] "Caches are synced" controller="node"
I0529 17:19:46.184787       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0529 17:19:46.184802       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0529 17:19:46.184805       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0529 17:19:46.184810       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0529 17:19:46.192185       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0529 17:19:46.196378       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0529 17:19:46.198744       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0529 17:19:46.200201       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0529 17:19:46.202686       1 shared_informer.go:357] "Caches are synced" controller="job"
I0529 17:19:46.212935       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0529 17:19:46.213008       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0529 17:19:46.214084       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0529 17:19:46.215459       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0529 17:19:46.215529       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0529 17:19:46.215574       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0529 17:19:46.216862       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0529 17:19:46.216951       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0529 17:19:46.217256       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0529 17:19:46.217312       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0529 17:19:46.219505       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0529 17:19:46.233741       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0529 17:19:46.280116       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0529 17:19:46.347225       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0529 17:19:46.364132       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0529 17:19:46.412929       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0529 17:19:46.413027       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0529 17:19:46.476516       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0529 17:19:46.494710       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0529 17:19:46.512355       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0529 17:19:46.512424       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0529 17:19:46.515413       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0529 17:19:46.519470       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0529 17:19:46.964444       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0529 17:19:46.964506       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0529 17:19:46.964522       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0529 17:19:46.980992       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"


==> kube-proxy [91ecfe17b909] <==
I0529 17:12:20.938198       1 server_linux.go:63] "Using iptables proxy"
I0529 17:12:21.209857       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0529 17:12:21.209942       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0529 17:12:21.224593       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0529 17:12:21.224638       1 server_linux.go:145] "Using iptables Proxier"
I0529 17:12:21.228316       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0529 17:12:21.235576       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0529 17:12:21.242348       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0529 17:12:21.242443       1 server.go:516] "Version info" version="v1.33.1"
I0529 17:12:21.242470       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0529 17:12:21.249389       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0529 17:12:21.255462       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0529 17:12:21.256299       1 config.go:199] "Starting service config controller"
I0529 17:12:21.256325       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0529 17:12:21.256363       1 config.go:105] "Starting endpoint slice config controller"
I0529 17:12:21.256376       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0529 17:12:21.256766       1 config.go:440] "Starting serviceCIDR config controller"
I0529 17:12:21.256791       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0529 17:12:21.268693       1 config.go:329] "Starting node config controller"
I0529 17:12:21.268750       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0529 17:12:21.357029       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0529 17:12:21.357046       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0529 17:12:21.357078       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0529 17:12:21.368942       1 shared_informer.go:357] "Caches are synced" controller="node config"


==> kube-proxy [968ee71859c4] <==
I0529 17:19:37.293852       1 server_linux.go:63] "Using iptables proxy"
I0529 17:19:42.190608       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0529 17:19:42.190918       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0529 17:19:42.397020       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0529 17:19:42.397189       1 server_linux.go:145] "Using iptables Proxier"
I0529 17:19:42.479078       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0529 17:19:42.489338       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0529 17:19:42.498386       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0529 17:19:42.498513       1 server.go:516] "Version info" version="v1.33.1"
I0529 17:19:42.498544       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0529 17:19:42.510561       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0529 17:19:42.522837       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0529 17:19:42.575787       1 config.go:329] "Starting node config controller"
I0529 17:19:42.575823       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0529 17:19:42.576927       1 config.go:199] "Starting service config controller"
I0529 17:19:42.576986       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0529 17:19:42.577148       1 config.go:105] "Starting endpoint slice config controller"
I0529 17:19:42.577166       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0529 17:19:42.577220       1 config.go:440] "Starting serviceCIDR config controller"
I0529 17:19:42.577231       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0529 17:19:42.679345       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0529 17:19:42.682833       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0529 17:19:42.682924       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0529 17:19:42.682944       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [0d3d74d75954] <==
I0529 17:19:40.693793       1 serving.go:386] Generated self-signed cert in-memory
W0529 17:19:42.005807       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0529 17:19:42.005865       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0529 17:19:42.005875       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0529 17:19:42.005881       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0529 17:19:42.280655       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0529 17:19:42.280686       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0529 17:19:42.285731       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0529 17:19:42.285807       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0529 17:19:42.286295       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0529 17:19:42.286838       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0529 17:19:42.386571       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [2ffddb0f7c78] <==
I0529 17:12:10.961194       1 serving.go:386] Generated self-signed cert in-memory
W0529 17:12:12.450635       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0529 17:12:12.450696       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0529 17:12:12.450710       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0529 17:12:12.450719       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0529 17:12:12.730780       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0529 17:12:12.730869       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0529 17:12:12.819055       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0529 17:12:12.819206       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0529 17:12:12.819982       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0529 17:12:12.820063       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0529 17:12:12.821734       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0529 17:12:12.823802       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0529 17:12:12.823893       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0529 17:12:12.824062       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0529 17:12:12.825678       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0529 17:12:12.825721       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0529 17:12:12.825777       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0529 17:12:12.825830       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0529 17:12:12.825856       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0529 17:12:12.825903       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0529 17:12:12.825936       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0529 17:12:12.825954       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0529 17:12:12.826021       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0529 17:12:12.826053       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0529 17:12:12.826115       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0529 17:12:12.826577       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0529 17:12:13.630738       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0529 17:12:13.661274       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0529 17:12:13.706570       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0529 17:12:13.716153       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0529 17:12:13.798427       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0529 17:12:13.953624       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0529 17:12:16.019551       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0529 17:19:16.981770       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
May 29 17:19:32 minikube kubelet[2674]: E0529 17:19:32.507194    2674 kuberuntime_manager.go:1161] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"6685efd1-f5dd-40b3-a17f-037fd9efc24a\" with KillPodSandboxError: \"rpc error: code = Unavailable desc = error reading from server: EOF\""
May 29 17:19:32 minikube kubelet[2674]: E0529 17:19:32.507272    2674 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"6685efd1-f5dd-40b3-a17f-037fd9efc24a\" with KillPodSandboxError: \"rpc error: code = Unavailable desc = error reading from server: EOF\"" pod="kube-system/coredns-674b8bbfcf-6znv6" podUID="6685efd1-f5dd-40b3-a17f-037fd9efc24a"
May 29 17:19:32 minikube kubelet[2674]: E0529 17:19:32.507265    2674 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"storage-provisioner_kube-system(93cadf6a-c068-45a5-9b7c-06c797d55e45)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"storage-provisioner_kube-system(93cadf6a-c068-45a5-9b7c-06c797d55e45)\\\": rpc error: code = Unavailable desc = error reading from server: EOF\"" pod="kube-system/storage-provisioner" podUID="93cadf6a-c068-45a5-9b7c-06c797d55e45"
May 29 17:19:33 minikube kubelet[2674]: E0529 17:19:33.045651    2674 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/events\": dial tcp 192.168.49.2:8443: connect: connection refused" event="&Event{ObjectMeta:{kube-scheduler-minikube.18440f1977707c2c  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:kube-system,Name:kube-scheduler-minikube,UID:feee622ba49882ef945e2406d3ba86df,APIVersion:v1,ResourceVersion:,FieldPath:spec.containers{kube-scheduler},},Reason:Unhealthy,Message:Readiness probe failed: Get \"https://127.0.0.1:10259/readyz\": dial tcp 127.0.0.1:10259: connect: connection refused,Source:EventSource{Component:kubelet,Host:minikube,},FirstTimestamp:2025-05-29 17:19:17.379054636 +0000 UTC m=+422.229334668,LastTimestamp:2025-05-29 17:19:17.379054636 +0000 UTC m=+422.229334668,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:minikube,}"
May 29 17:19:33 minikube kubelet[2674]: E0529 17:19:33.477721    2674 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://192.168.49.2:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="6.4s"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.491396    2674 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="41704ac52cd10cad0d747aa5ece69fae0599e6235cb4e325a3197c789390aaaf"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.496381    2674 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f66e22dd1407df1e5dafddf4c28bc45e776d0cf72e57f2e7ecec704f4b334816"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.501448    2674 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b642a2d8b2b3cbe1ee9a684085a2db675e684990042c1c763043aa80aac73e9b"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.501980    2674 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.502191    2674 status_manager.go:895] "Failed to get status for pod" podUID="0378f173c980f85a71d36305bacb0ad1" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.502348    2674 status_manager.go:895] "Failed to get status for pod" podUID="6685efd1-f5dd-40b3-a17f-037fd9efc24a" pod="kube-system/coredns-674b8bbfcf-6znv6" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/coredns-674b8bbfcf-6znv6\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.502566    2674 status_manager.go:895] "Failed to get status for pod" podUID="c539c3b2-939f-4dce-943b-783e180f5a55" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-5txqq" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kubernetes-dashboard/pods/dashboard-metrics-scraper-5d59dccf9b-5txqq\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.502735    2674 status_manager.go:895] "Failed to get status for pod" podUID="3924ef3609584191d8d09190210d2d78" pod="kube-system/etcd-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.502925    2674 status_manager.go:895] "Failed to get status for pod" podUID="78e1292e1d47cc7d09b2c6f5826fa624" pod="kube-system/kube-apiserver-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.503111    2674 status_manager.go:895] "Failed to get status for pod" podUID="93cadf6a-c068-45a5-9b7c-06c797d55e45" pod="kube-system/storage-provisioner" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.503213    2674 status_manager.go:895] "Failed to get status for pod" podUID="8107a677-6260-4718-8b5c-0396679d0867" pod="kube-system/kube-proxy-4qqb8" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-proxy-4qqb8\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.503387    2674 status_manager.go:895] "Failed to get status for pod" podUID="f53e3fdd-c9c5-43e3-8ee5-82b0ac9218b3" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-xlvjx" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kubernetes-dashboard/pods/kubernetes-dashboard-7779f9b69b-xlvjx\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.506576    2674 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7c18a59d37a28e47dd7d22829d72dae781512f3e31398caf2317aaf52dc08841"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.512194    2674 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ceafea647422a9d87ddb19cbcc5b2b9b1a7d702560b3ae0f371710b7849669c3"
May 29 17:19:33 minikube kubelet[2674]: I0529 17:19:33.517795    2674 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7d3b7fe5cb78f5bac27c590d02d12bb6c42bbe66066f4ffa958a3590982da1d1"
May 29 17:19:34 minikube kubelet[2674]: I0529 17:19:34.706127    2674 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="82b624e8ab2b443ec553aa60d30056dbfe316e02c2dc0052812b741efcf84a86"
May 29 17:19:34 minikube kubelet[2674]: I0529 17:19:34.727183    2674 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="abdf896247354907413aee5d8b6257d8a68fa67953b0bd6ba017074973c38e4d"
May 29 17:19:34 minikube kubelet[2674]: I0529 17:19:34.877317    2674 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="feeeb82b175a8ef3cde8bf407555e66241b9cc285458a8335e425dc8e9106246"
May 29 17:19:34 minikube kubelet[2674]: I0529 17:19:34.886457    2674 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="db9889e566698eea9fa91a5ffee87ce2807ac3a2b6e6298432f673287e7ecc59"
May 29 17:19:34 minikube kubelet[2674]: I0529 17:19:34.894150    2674 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="13861c4337f8a81d5f91c0afbf916889ad103addc30cfefcd80fa9e3b4f6ac5c"
May 29 17:19:35 minikube kubelet[2674]: I0529 17:19:35.377681    2674 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:35 minikube kubelet[2674]: I0529 17:19:35.378674    2674 status_manager.go:895] "Failed to get status for pod" podUID="0378f173c980f85a71d36305bacb0ad1" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:35 minikube kubelet[2674]: I0529 17:19:35.379414    2674 status_manager.go:895] "Failed to get status for pod" podUID="6685efd1-f5dd-40b3-a17f-037fd9efc24a" pod="kube-system/coredns-674b8bbfcf-6znv6" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/coredns-674b8bbfcf-6znv6\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:35 minikube kubelet[2674]: I0529 17:19:35.380104    2674 status_manager.go:895] "Failed to get status for pod" podUID="c539c3b2-939f-4dce-943b-783e180f5a55" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-5txqq" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kubernetes-dashboard/pods/dashboard-metrics-scraper-5d59dccf9b-5txqq\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:35 minikube kubelet[2674]: I0529 17:19:35.380835    2674 status_manager.go:895] "Failed to get status for pod" podUID="3924ef3609584191d8d09190210d2d78" pod="kube-system/etcd-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:35 minikube kubelet[2674]: I0529 17:19:35.381593    2674 status_manager.go:895] "Failed to get status for pod" podUID="78e1292e1d47cc7d09b2c6f5826fa624" pod="kube-system/kube-apiserver-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:35 minikube kubelet[2674]: I0529 17:19:35.383045    2674 status_manager.go:895] "Failed to get status for pod" podUID="93cadf6a-c068-45a5-9b7c-06c797d55e45" pod="kube-system/storage-provisioner" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:35 minikube kubelet[2674]: I0529 17:19:35.383700    2674 status_manager.go:895] "Failed to get status for pod" podUID="8107a677-6260-4718-8b5c-0396679d0867" pod="kube-system/kube-proxy-4qqb8" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-proxy-4qqb8\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:35 minikube kubelet[2674]: I0529 17:19:35.384534    2674 status_manager.go:895] "Failed to get status for pod" podUID="f53e3fdd-c9c5-43e3-8ee5-82b0ac9218b3" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-xlvjx" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kubernetes-dashboard/pods/kubernetes-dashboard-7779f9b69b-xlvjx\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:36 minikube kubelet[2674]: I0529 17:19:36.277679    2674 status_manager.go:895] "Failed to get status for pod" podUID="3924ef3609584191d8d09190210d2d78" pod="kube-system/etcd-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:36 minikube kubelet[2674]: I0529 17:19:36.278296    2674 status_manager.go:895] "Failed to get status for pod" podUID="78e1292e1d47cc7d09b2c6f5826fa624" pod="kube-system/kube-apiserver-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:36 minikube kubelet[2674]: I0529 17:19:36.278647    2674 status_manager.go:895] "Failed to get status for pod" podUID="93cadf6a-c068-45a5-9b7c-06c797d55e45" pod="kube-system/storage-provisioner" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:36 minikube kubelet[2674]: I0529 17:19:36.278940    2674 status_manager.go:895] "Failed to get status for pod" podUID="8107a677-6260-4718-8b5c-0396679d0867" pod="kube-system/kube-proxy-4qqb8" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-proxy-4qqb8\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:36 minikube kubelet[2674]: I0529 17:19:36.279243    2674 status_manager.go:895] "Failed to get status for pod" podUID="f53e3fdd-c9c5-43e3-8ee5-82b0ac9218b3" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-xlvjx" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kubernetes-dashboard/pods/kubernetes-dashboard-7779f9b69b-xlvjx\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:36 minikube kubelet[2674]: I0529 17:19:36.279502    2674 status_manager.go:895] "Failed to get status for pod" podUID="feee622ba49882ef945e2406d3ba86df" pod="kube-system/kube-scheduler-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:36 minikube kubelet[2674]: I0529 17:19:36.279971    2674 status_manager.go:895] "Failed to get status for pod" podUID="0378f173c980f85a71d36305bacb0ad1" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:36 minikube kubelet[2674]: I0529 17:19:36.280584    2674 status_manager.go:895] "Failed to get status for pod" podUID="6685efd1-f5dd-40b3-a17f-037fd9efc24a" pod="kube-system/coredns-674b8bbfcf-6znv6" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/pods/coredns-674b8bbfcf-6znv6\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:36 minikube kubelet[2674]: I0529 17:19:36.281269    2674 status_manager.go:895] "Failed to get status for pod" podUID="c539c3b2-939f-4dce-943b-783e180f5a55" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-5txqq" err="Get \"https://192.168.49.2:8443/api/v1/namespaces/kubernetes-dashboard/pods/dashboard-metrics-scraper-5d59dccf9b-5txqq\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 29 17:19:37 minikube kubelet[2674]: I0529 17:19:37.788685    2674 scope.go:117] "RemoveContainer" containerID="4b781dd0325b23ad3b7850e7861093cce3e35160ed103217e28855562d509434"
May 29 17:19:37 minikube kubelet[2674]: E0529 17:19:37.789175    2674 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(93cadf6a-c068-45a5-9b7c-06c797d55e45)\"" pod="kube-system/storage-provisioner" podUID="93cadf6a-c068-45a5-9b7c-06c797d55e45"
May 29 17:19:39 minikube kubelet[2674]: I0529 17:19:39.882070    2674 scope.go:117] "RemoveContainer" containerID="ebafcbfd70211090ec0764b3fe10befbff446a0209ec0cee127792ee52159b4f"
May 29 17:19:39 minikube kubelet[2674]: I0529 17:19:39.882536    2674 scope.go:117] "RemoveContainer" containerID="4b781dd0325b23ad3b7850e7861093cce3e35160ed103217e28855562d509434"
May 29 17:19:39 minikube kubelet[2674]: E0529 17:19:39.882742    2674 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(93cadf6a-c068-45a5-9b7c-06c797d55e45)\"" pod="kube-system/storage-provisioner" podUID="93cadf6a-c068-45a5-9b7c-06c797d55e45"
May 29 17:19:42 minikube kubelet[2674]: I0529 17:19:42.005267    2674 status_manager.go:895] "Failed to get status for pod" podUID="78e1292e1d47cc7d09b2c6f5826fa624" pod="kube-system/kube-apiserver-minikube" err="pods \"kube-apiserver-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
May 29 17:19:42 minikube kubelet[2674]: E0529 17:19:42.005813    2674 reflector.go:200] "Failed to watch" err="configmaps \"kube-root-ca.crt\" is forbidden: User \"system:node:minikube\" cannot watch resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" logger="UnhandledError" reflector="object-\"kube-system\"/\"kube-root-ca.crt\"" type="*v1.ConfigMap"
May 29 17:19:42 minikube kubelet[2674]: I0529 17:19:42.083410    2674 status_manager.go:895] "Failed to get status for pod" podUID="93cadf6a-c068-45a5-9b7c-06c797d55e45" pod="kube-system/storage-provisioner" err=<
May 29 17:19:42 minikube kubelet[2674]:         pods "storage-provisioner" is forbidden: User "system:node:minikube" cannot get resource "pods" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
May 29 17:19:42 minikube kubelet[2674]:         RBAC: [clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:certificates.k8s.io:certificatesigningrequests:selfnodeclient" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found]
May 29 17:19:42 minikube kubelet[2674]:  >
May 29 17:19:42 minikube kubelet[2674]: I0529 17:19:42.086297    2674 status_manager.go:895] "Failed to get status for pod" podUID="8107a677-6260-4718-8b5c-0396679d0867" pod="kube-system/kube-proxy-4qqb8" err="pods \"kube-proxy-4qqb8\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
May 29 17:19:54 minikube kubelet[2674]: I0529 17:19:54.297040    2674 scope.go:117] "RemoveContainer" containerID="4b781dd0325b23ad3b7850e7861093cce3e35160ed103217e28855562d509434"
May 29 17:19:54 minikube kubelet[2674]: E0529 17:19:54.297417    2674 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(93cadf6a-c068-45a5-9b7c-06c797d55e45)\"" pod="kube-system/storage-provisioner" podUID="93cadf6a-c068-45a5-9b7c-06c797d55e45"
May 29 17:20:06 minikube kubelet[2674]: I0529 17:20:06.294734    2674 scope.go:117] "RemoveContainer" containerID="4b781dd0325b23ad3b7850e7861093cce3e35160ed103217e28855562d509434"
May 29 17:22:18 minikube kubelet[2674]: I0529 17:22:18.822670    2674 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-f2cm8\" (UniqueName: \"kubernetes.io/projected/da8d69ca-f38e-4a2b-9acf-a6795346a6ec-kube-api-access-f2cm8\") pod \"hello-node-c74958b5d-q6rxz\" (UID: \"da8d69ca-f38e-4a2b-9acf-a6795346a6ec\") " pod="default/hello-node-c74958b5d-q6rxz"
May 29 17:22:40 minikube kubelet[2674]: I0529 17:22:40.209055    2674 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/hello-node-c74958b5d-q6rxz" podStartSLOduration=2.110839541 podStartE2EDuration="22.209009932s" podCreationTimestamp="2025-05-29 17:22:18 +0000 UTC" firstStartedPulling="2025-05-29 17:22:19.17263382 +0000 UTC m=+604.036033428" lastFinishedPulling="2025-05-29 17:22:39.2678604 +0000 UTC m=+624.134203819" observedRunningTime="2025-05-29 17:22:40.2087991 +0000 UTC m=+625.075142523" watchObservedRunningTime="2025-05-29 17:22:40.209009932 +0000 UTC m=+625.075353356"


==> kubernetes-dashboard [529a489c35cb] <==
2025/05/29 17:14:52 [2025-05-29T17:14:52Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:14:52 Getting list of all replication controllers in the cluster
2025/05/29 17:14:52 [2025-05-29T17:14:52Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:14:52 [2025-05-29T17:14:52Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:14:52 Getting list of all pet sets in the cluster
2025/05/29 17:14:52 [2025-05-29T17:14:52Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:04 Getting list of all cron jobs in the cluster
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:04 Getting list of all deployments in the cluster
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/05/29 17:15:04 Getting list of namespaces
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:04 Getting list of all pods in the cluster
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:04 Getting list of all jobs in the cluster
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:04 Getting pod metrics
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:04 Getting list of all replica sets in the cluster
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:04 Getting list of all pet sets in the cluster
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:04 Getting list of all replication controllers in the cluster
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:04 [2025-05-29T17:15:04Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:08 [2025-05-29T17:15:08Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/05/29 17:15:08 Getting list of namespaces
2025/05/29 17:15:08 [2025-05-29T17:15:08Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:08 Getting list of all cron jobs in the cluster
2025/05/29 17:15:08 [2025-05-29T17:15:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:08 [2025-05-29T17:15:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:09 [2025-05-29T17:15:09Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:09 [2025-05-29T17:15:09Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:09 Getting list of all deployments in the cluster
2025/05/29 17:15:09 [2025-05-29T17:15:09Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:09 Getting list of all jobs in the cluster
2025/05/29 17:15:09 [2025-05-29T17:15:09Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:09 [2025-05-29T17:15:09Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:09 [2025-05-29T17:15:09Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:10 [2025-05-29T17:15:10Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:10 Getting list of all replica sets in the cluster
2025/05/29 17:15:10 [2025-05-29T17:15:10Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:10 Getting list of all pods in the cluster
2025/05/29 17:15:10 [2025-05-29T17:15:10Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:10 Getting list of all replication controllers in the cluster
2025/05/29 17:15:10 [2025-05-29T17:15:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:10 Getting pod metrics
2025/05/29 17:15:10 [2025-05-29T17:15:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:10 [2025-05-29T17:15:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:15:11 [2025-05-29T17:15:11Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:15:11 Getting list of all pet sets in the cluster
2025/05/29 17:15:11 [2025-05-29T17:15:11Z] Outcoming response to 127.0.0.1 with 200 status code


==> kubernetes-dashboard [94981d1fd38c] <==
2025/05/29 17:41:34 [2025-05-29T17:41:34Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:34 [2025-05-29T17:41:34Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:41:34 Getting list of all replica sets in the cluster
2025/05/29 17:41:34 [2025-05-29T17:41:34Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:41:34 Getting list of all replication controllers in the cluster
2025/05/29 17:41:34 received 0 resources from sidecar instead of 1
2025/05/29 17:41:34 [2025-05-29T17:41:34Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:41:34 Getting list of all pet sets in the cluster
2025/05/29 17:41:34 received 0 resources from sidecar instead of 1
2025/05/29 17:41:34 [2025-05-29T17:41:34Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:34 received 0 resources from sidecar instead of 1
2025/05/29 17:41:34 [2025-05-29T17:41:34Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:34 [2025-05-29T17:41:34Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:34 received 0 resources from sidecar instead of 1
2025/05/29 17:41:34 Getting pod metrics
2025/05/29 17:41:34 received 0 resources from sidecar instead of 1
2025/05/29 17:41:34 received 0 resources from sidecar instead of 1
2025/05/29 17:41:34 received 0 resources from sidecar instead of 1
2025/05/29 17:41:34 [2025-05-29T17:41:34Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:34 received 0 resources from sidecar instead of 1
2025/05/29 17:41:34 Skipping metric because of error: Metric label not set.
2025/05/29 17:41:34 Skipping metric because of error: Metric label not set.
2025/05/29 17:41:34 [2025-05-29T17:41:34Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:39 [2025-05-29T17:41:39Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/05/29 17:41:39 Getting list of namespaces
2025/05/29 17:41:39 [2025-05-29T17:41:39Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:41:39 [2025-05-29T17:41:39Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:39 Getting list of all cron jobs in the cluster
2025/05/29 17:41:39 [2025-05-29T17:41:39Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:40 [2025-05-29T17:41:40Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:41:40 [2025-05-29T17:41:40Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:41:40 Getting list of all deployments in the cluster
2025/05/29 17:41:40 [2025-05-29T17:41:40Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:40 [2025-05-29T17:41:40Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:41:40 Getting list of all jobs in the cluster
2025/05/29 17:41:40 [2025-05-29T17:41:40Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:40 received 0 resources from sidecar instead of 1
2025/05/29 17:41:40 received 0 resources from sidecar instead of 1
2025/05/29 17:41:40 [2025-05-29T17:41:40Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:41 [2025-05-29T17:41:41Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:41:41 Getting list of all replica sets in the cluster
2025/05/29 17:41:41 [2025-05-29T17:41:41Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:41:41 Getting list of all pods in the cluster
2025/05/29 17:41:41 [2025-05-29T17:41:41Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:41:41 Getting list of all replication controllers in the cluster
2025/05/29 17:41:41 [2025-05-29T17:41:41Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/29 17:41:41 Getting list of all pet sets in the cluster
2025/05/29 17:41:41 [2025-05-29T17:41:41Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:41 received 0 resources from sidecar instead of 1
2025/05/29 17:41:41 [2025-05-29T17:41:41Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:41 received 0 resources from sidecar instead of 1
2025/05/29 17:41:41 received 0 resources from sidecar instead of 1
2025/05/29 17:41:41 [2025-05-29T17:41:41Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/29 17:41:41 received 0 resources from sidecar instead of 1
2025/05/29 17:41:41 Getting pod metrics
2025/05/29 17:41:41 received 0 resources from sidecar instead of 1
2025/05/29 17:41:42 received 0 resources from sidecar instead of 1
2025/05/29 17:41:42 Skipping metric because of error: Metric label not set.
2025/05/29 17:41:42 Skipping metric because of error: Metric label not set.
2025/05/29 17:41:42 [2025-05-29T17:41:42Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [4b781dd0325b] <==
I0529 17:19:36.486011       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0529 17:19:36.576202       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [a59b13db9bae] <==
W0529 17:42:33.769690       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:33.775878       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:35.780475       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:35.786080       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:37.790724       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:37.797986       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:39.800825       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:39.806351       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:41.813079       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:41.821532       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:43.823712       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:43.828553       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:45.833270       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:45.841570       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:47.845246       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:47.849420       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:49.852408       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:49.858965       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:51.862223       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:51.867652       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:53.871602       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:53.880327       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:55.888812       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:55.897143       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:57.901946       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:57.908014       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:59.913980       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:42:59.926275       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:01.929538       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:01.933272       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:03.933411       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:03.940807       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:05.947461       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:05.955904       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:07.960975       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:07.967683       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:09.971635       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:09.976530       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:11.979535       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:11.985629       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:13.989169       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:13.994509       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:15.997976       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:16.004019       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:18.006740       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:18.016679       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:20.020340       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:20.026018       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:22.030678       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:22.039380       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:24.046084       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:24.057124       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:26.061194       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:26.068153       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:28.072043       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:28.077033       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:30.080963       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:30.089560       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:32.091697       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0529 17:43:32.095438       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

